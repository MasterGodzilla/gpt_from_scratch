{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = { ch:i for i, ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "def encode(s):\n",
    "    return [char_to_idx[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([idx_to_char[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"hii\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(0.9*len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    return train_data, val_data\n",
    "\n",
    "train_data, val_data = load_data()\n",
    "print(train_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current device: cuda:3\n",
      "Default dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "# Add after device definition (around in[6])\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {device}\")\n",
    "print(f\"Default dtype: {torch.get_default_dtype()}\")\n",
    "\n",
    "# To enable BF16:\n",
    "torch.set_default_dtype(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "YORK:\n",
      "Wh\n",
      "ORK:\n",
      "Wha\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_batch(batch_size, block_size):\n",
    "    random_indices = torch.randint(len(train_data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in random_indices])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in random_indices])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "def prefetch_batches(num_batches, batch_size, block_size):\n",
    "    batches = []\n",
    "    xb, yb = load_batch(batch_size=batch_size*num_batches, block_size=block_size)\n",
    "    # Reshape to split into num_batches\n",
    "    xb = xb.view(num_batches, batch_size, -1) \n",
    "    yb = yb.view(num_batches, batch_size, -1)\n",
    "    batches = list(zip(xb, yb))\n",
    "    return batches\n",
    "\n",
    "xb, yb = load_batch(batch_size=32, block_size=8)\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "print(decode(xb[0].tolist()))\n",
    "print(decode(yb[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.tril = torch.tril(torch.ones_like(torch.zeros(block_size, block_size))).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weight = q @ k.transpose(-2, -1) * (C**-0.5) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # put upper triangular part of weight to -inf\n",
    "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        v = weight @ v\n",
    "        return v\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(n_embd, head_size, block_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        self.sa = MHA(n_embd, n_heads, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # Keep LayerNorm in fp32\n",
    "        self.ln1 = nn.LayerNorm(n_embd).to(torch.float32)\n",
    "        self.ln2 = nn.LayerNorm(n_embd).to(torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert to fp32 for LayerNorm, then back to bf16\n",
    "        x = self.ln1(x.to(torch.float32)).to(torch.bfloat16)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.ln2(x.to(torch.float32)).to(torch.bfloat16)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, n_blocks, block_size):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_heads = n_heads\n",
    "        self.n_blocks = n_blocks\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads, block_size) for _ in range(n_blocks)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, T = 1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            if T > 0.01:\n",
    "                logits = logits / T\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else: # greedy sampling\n",
    "                idx_next = logits.argmax(dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    # class method\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, filepath, n_embd, n_heads, n_blocks, block_size):\n",
    "        \"\"\"Load model from a checkpoint file\"\"\"\n",
    "        model = cls(n_embd=n_embd, n_heads=n_heads, n_blocks=n_blocks, block_size=block_size)\n",
    "        model.load_state_dict(torch.load(filepath))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model, batch_size, block_size):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        X, Y = load_batch(batch_size, block_size)\n",
    "        logits, loss = model(X, Y)\n",
    "        out[split] = loss.item()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2644269/3940771281.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? How are you doing?\n",
      "Is the my deeds deliver'd up from the diadem.\n",
      "O God! if you be so?\n",
      "\n",
      "KING RICHARD III:\n",
      "O, nothing pi\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "n_blocks = 8\n",
    "n_heads = 8\n",
    "n_embd = 128\n",
    "\n",
    "\n",
    "# try inference\n",
    "# model = GPT(n_embd=n_embd, n_heads=n_heads, n_blocks=n_blocks, block_size=block_size).to(device)\n",
    "model = GPT.from_checkpoint(\"model_19500.pth\", n_embd, n_heads, n_blocks, block_size).to(device)\n",
    "# model.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? How are you doing?\n",
      "If thou depart to come the law of them?\n",
      "\n",
      "LUCIO:\n",
      "I warrant thee, for I have heard him for his country\n",
      "And manners that the sea mock'd the babe,\n",
      "And the deceived of my strength weak a far\n",
      "And see him show me on him.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "I will be king, and there be still.\n",
      "\n",
      "CAMILLO:\n",
      "Sir, my lord, this is \n"
     ]
    }
   ],
   "source": [
    "message = \"Hello, how are you? How are you doing?\"\n",
    "idx = torch.tensor(encode(message), dtype=torch.long).unsqueeze(0).to(device)\n",
    "print(decode(model.generate(idx, max_new_tokens=300, T=0.3)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = 1e-3  # Peak learning rate\n",
    "lr_min = 1e-4  # Minimum learning rate\n",
    "total_iters = 20000\n",
    "warmup_iters = 0  # Number of warmup iterations\n",
    "lr_decay_iters = total_iters  # Total number of iterations for lr decay\n",
    "anneal_iters = 500\n",
    "\n",
    "def get_lr(it):\n",
    "    # Linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return lr_max * it / warmup_iters\n",
    "    # Cosine learning rate decay\n",
    "    if it > lr_decay_iters:\n",
    "        return lr_min\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # Cosine decay\n",
    "    return lr_min + coeff * (lr_max - lr_min)\n",
    "\n",
    "def trapezoid_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return lr_max * it / warmup_iters\n",
    "    if it > total_iters - anneal_iters:\n",
    "        return lr_max - (it - (total_iters - anneal_iters)) * (lr_max - lr_min) / anneal_iters\n",
    "    return lr_max\n",
    "\n",
    "\n",
    "# Modify training loop\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 1.1986 (elapsed: 50.62s) lr 0.0010\n",
      "example output: Hello, how are you? How are you doing?\n",
      "son that shun's musil'd too blowshin, though noney\n",
      "tones ouncious plotshion, a roblion\n",
      "ttone, and t\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "import time\n",
    "from torch.amp import autocast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "batch_size = 128\n",
    "eval_interval = 100\n",
    "prefetch_size = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "losses = []\n",
    "learning_rates = []\n",
    "\n",
    "start_time = time.time()\n",
    "batches = prefetch_batches(prefetch_size, batch_size, block_size)\n",
    "for i in range(total_iters):\n",
    "\n",
    "    lr = trapezoid_lr(i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    learning_rates.append(lr)\n",
    "\n",
    "    batch_idx = i % prefetch_size\n",
    "    if batch_idx == 0:\n",
    "        batches = prefetch_batches(prefetch_size, batch_size, block_size)\n",
    "    xb, yb = batches[batch_idx]\n",
    "    with autocast('cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"iter {i}: loss {loss.item():.4f} (elapsed: {elapsed:.2f}s) lr {lr:.4f}\")\n",
    "        print(\"example output:\", decode(model.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "        out = estimate_loss(model, batch_size, block_size)\n",
    "        print(f\"train loss: {out['train']:.4f}, val loss: {out['val']:.4f}\")\n",
    "    \n",
    "    # save a checkpoint before annealing\n",
    "    if i == total_iters - anneal_iters:\n",
    "        torch.save(model.state_dict(), f\"model_{i}.pth\")\n",
    "# Plot loss and learning rate\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "ax1.plot(losses)\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "ax2.plot(learning_rates)\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.set_xlabel('Iteration') \n",
    "ax2.set_ylabel('Learning Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not unnatural chance: the can bite\n",
      "yet: but yet gone to all the struck in exile;\n",
      "Condemns thee humour fly: where was he move\n",
      "homely--as now my mother, my mouse will gaze him,\n",
      "Mumusic, O see me bid prince disobed,\n",
      "And weak straight-westeed were substitute\n",
      "From thyself to sweat but a suit: how some stone\n"
     ]
    }
   ],
   "source": [
    "message = \"To be or not\"\n",
    "idx = torch.tensor(encode(message), dtype=torch.long).unsqueeze(0).to(device)\n",
    "print (decode(model.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
